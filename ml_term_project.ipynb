{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# Term Project: Implementing ML Models from Scratch\n",
        "## Machine Learning / Pattern Recognition\n",
        "### FINAL INTERNATIONAL UNIVERSITY | Fall 2025-26\n",
        "\n",
        "**Student Name:HAKIZIMANA TONY CARLIN**  \n",
        "**Student ID:2510140033**  \n",
        "**Due Date:** 27 December 2025 (Saturday) before midnight\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "overview"
      },
      "source": [
        "## Overview\n",
        "\n",
        "In this project, you will implement three fundamental machine learning models **from scratch**:\n",
        "1. **Linear Regression** using gradient descent\n",
        "2. **Logistic Regression** using gradient descent (binary classification)\n",
        "3. **Decision Tree** using information gain (multi-class classification)\n",
        "\n",
        "You will use the **Iris dataset** for all three tasks:\n",
        "- **Linear Regression**: Predict `petal_width` from other features\n",
        "- **Logistic Regression**: Classify `setosa` vs `non-setosa` (binary)\n",
        "- **Decision Tree**: Classify all three species (multi-class)\n",
        "\n",
        "**Important Rules:**\n",
        "- ✅ **Allowed**: `sklearn` for dataset loading, `KFold`, and evaluation metrics\n",
        "- ❌ **Not Allowed**: `sklearn` models (e.g., `LinearRegression`, `LogisticRegression`, `DecisionTreeClassifier`)\n",
        "- ❌ **Not Allowed**: `GridSearchCV` or similar libraries for hyperparameter search\n",
        "- All models must be implemented using **native Python and NumPy only**\n",
        "\n",
        "**Grading:**\n",
        "- Linear Regression Implementation: **20 pts**\n",
        "- Linear Regression 5-Fold CV & Results: **10 pts**\n",
        "- Logistic Regression Implementation: **20 pts**\n",
        "- Logistic Regression 5-Fold CV & Results: **10 pts**\n",
        "- Decision Tree Implementation: **20 pts**\n",
        "- Decision Tree 5-Fold CV & Results: **10 pts**\n",
        "- Written Questions (3 × 4 pts): **12 pts** \n",
        "- **Total: 100 + 2 pts**\n",
        "\n",
        "**Integrity Test:**\n",
        "- **Date**: 3 January 2026 during lecture hours\n",
        "- **Format**: Short quiz on paper\n",
        "- **Pass (≥80%)**: Marks awarded directly from PDF\n",
        "- **Fail (<80%)**: Demo session required\n",
        "- **No Show**: 50% penalty"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "checklist"
      },
      "source": [
        "---\n",
        "\n",
        "## Pre-Submission Checklist\n",
        "\n",
        "- [ ] Name and student ID at top\n",
        "- [ ] No cells are added or removed\n",
        "- [ ] All TODO sections completed\n",
        "- [ ] All questions answered\n",
        "- [ ] Code runs without errors\n",
        "- [ ] Results tables included with mean, std, and 95% CI\n",
        "- [ ] Hyperparameter tuning plots included\n",
        "- [ ] Run All before saving"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup_header"
      },
      "source": [
        "## Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "imports"
      },
      "outputs": [],
      "source": [
        "# Standard libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "\n",
        "# Sklearn - ONLY for dataset, splitting, and metrics\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "seed = 42\n",
        "np.random.seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "data_header"
      },
      "source": [
        "## Load and Prepare Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "load_data"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset Information:\n",
            "  Total samples: 150\n",
            "  Features: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
            "  Classes: ['setosa' 'versicolor' 'virginica']\n",
            "\n",
            "Class distribution: {np.str_('setosa'): np.int64(50), np.str_('versicolor'): np.int64(50), np.str_('virginica'): np.int64(50)}\n"
          ]
        }
      ],
      "source": [
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X_full = iris.data  # Features: sepal_length, sepal_width, petal_length, petal_width\n",
        "y_full = iris.target  # Labels: 0=setosa, 1=versicolor, 2=virginica\n",
        "\n",
        "print(\"Dataset Information:\")\n",
        "print(f\"  Total samples: {X_full.shape[0]}\")\n",
        "print(f\"  Features: {iris.feature_names}\")\n",
        "print(f\"  Classes: {iris.target_names}\")\n",
        "print(f\"\\nClass distribution: {dict(zip(iris.target_names, np.bincount(y_full)))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "helper_header"
      },
      "source": [
        "## Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "helpers"
      },
      "outputs": [],
      "source": [
        "def compute_confidence_interval(values, confidence=0.95):\n",
        "    \"\"\"\n",
        "    Compute mean, std, and 95% confidence interval.\n",
        "    \n",
        "    Args:\n",
        "        values: List or array of values from k folds\n",
        "        confidence: Confidence level (default 0.95)\n",
        "    \n",
        "    Returns:\n",
        "        mean, std, (lower_bound, upper_bound)\n",
        "    \"\"\"\n",
        "    values = np.array(values)\n",
        "    n = len(values)\n",
        "    mean = np.mean(values)\n",
        "    std = np.std(values, ddof=1)  # Sample std\n",
        "    \n",
        "    # t-value for 95% CI with n-1 degrees of freedom (approx 2.776 for n=5)\n",
        "    t_value = 2.776  # For 5 folds, df=4\n",
        "    margin = t_value * (std / np.sqrt(n))\n",
        "    \n",
        "    return mean, std, (mean - margin, mean + margin)\n",
        "\n",
        "\n",
        "def print_results(metric_name, values):\n",
        "    \"\"\"Print results with mean, std, and 95% CI.\"\"\"\n",
        "    mean, std, ci = compute_confidence_interval(values)\n",
        "    print(f\"{metric_name}: {mean:.4f} ± {std:.4f} (95% CI: [{ci[0]:.4f}, {ci[1]:.4f}])\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "task1_header"
      },
      "source": [
        "---\n",
        "\n",
        "# Task 1: Linear Regression (30 points)\n",
        "\n",
        "Implement Linear Regression using gradient descent to predict `petal_width` from the other three features (`sepal_length`, `sepal_width`, `petal_length`).\n",
        "\n",
        "**Hyperparameters to tune:**\n",
        "- Learning rate: `[0.001, 0.01, 0.1]`\n",
        "- Regularisation strength (L2): `[0.01, 0.1]`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "linreg_class_header"
      },
      "source": [
        "## 1.1: Implement LinearRegression Class (20 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "linreg_class"
      },
      "outputs": [],
      "source": [
        "class LinearRegression:\n",
        "    \"\"\"\n",
        "    Linear Regression using gradient descent.\n",
        "    \n",
        "    Attributes:\n",
        "        learning_rate: Step size for gradient descent\n",
        "        n_iterations: Number of training iterations (fixed = 500)\n",
        "        reg_strength: L2 regularisation strength (0 = no regularisation)\n",
        "        weights: Model weights (including bias)\n",
        "        train_losses: List of training losses per iteration\n",
        "        val_losses: List of validation losses per iteration\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, n_iterations=500, learning_rate=0.01, reg_strength=0.0):\n",
        "        \"\"\"\n",
        "        Initialise the Linear Regression model.\n",
        "        \n",
        "        Args:\n",
        "            learning_rate: Step size for gradient descent\n",
        "            n_iterations: Number of training iterations\n",
        "            reg_strength: L2 regularisation strength\n",
        "        \"\"\"\n",
        "        # TODO: Store hyperparameters and initialise attributes (3 points)\n",
        "        self.learning_rate = learning_rate\n",
        "        self.n_iterations = n_iterations\n",
        "        self.reg_strength = reg_strength\n",
        "        self.weights = None\n",
        "        self.train_losses = []\n",
        "        self.val_losses = []\n",
        "    \n",
        "    def _add_bias(self, X):\n",
        "        \"\"\"Add a column of ones for the bias term.\"\"\"\n",
        "        # TODO: Add bias column to X (2 points)\n",
        "        # Hint: Use np.c_ or np.column_stack\n",
        "        return np.c_[np.ones(X.shape[0]), X]\n",
        "    \n",
        "    def _compute_loss(self, X, y):\n",
        "        \"\"\"\n",
        "        Compute Mean Squared Error loss with optional L2 regularisation.\n",
        "        \n",
        "        Loss = (1/2n) * sum((y_pred - y)^2) + (reg_strength/2) * sum(weights^2)\n",
        "        \n",
        "        Note: Do not regularise the bias term.\n",
        "        \"\"\"\n",
        "        # TODO: Implement MSE loss with L2 regularisation (2 points)\n",
        "        y_pred = X @ self.weights\n",
        "        mse_loss = (1 / (2 * len(y))) * np.sum((y_pred - y) ** 2)\n",
        "        reg_loss = (self.reg_strength / 2) * np.sum(self.weights[1:] ** 2)  # Exclude bias\n",
        "        return mse_loss + reg_loss\n",
        "    \n",
        "    def _compute_gradient(self, X, y):\n",
        "        \"\"\"\n",
        "        Compute gradient of the loss function.\n",
        "        \n",
        "        Gradient = (1/n) * X.T @ (X @ weights - y) + reg_strength * weights\n",
        "        \n",
        "        Note: Do not regularise the bias term.\n",
        "        \"\"\"\n",
        "        # TODO: Implement gradient computation (2 points)\n",
        "        y_pred = X @ self.weights\n",
        "        gradient = (1 / len(y)) * X.T @ (y_pred - y)\n",
        "        gradient[1:] += self.reg_strength * self.weights[1:]  # Don't regularize bias\n",
        "        return gradient\n",
        "    \n",
        "    def fit(self, X_train, y_train, X_val=None, y_val=None):\n",
        "        \"\"\"\n",
        "        Train the model using gradient descent.\n",
        "        \n",
        "        Args:\n",
        "            X_train: Training features\n",
        "            y_train: Training targets\n",
        "            X_val: Validation features (optional, for tracking val loss)\n",
        "            y_val: Validation targets (optional)\n",
        "        \n",
        "        Returns:\n",
        "            self\n",
        "        \"\"\"\n",
        "        # TODO: Implement training loop (8 points)\n",
        "        # Steps:\n",
        "        # 1. Add bias to X_train (and X_val if provided)\n",
        "        # 2. Initialise weights to zeros\n",
        "        # 3. For each iteration:\n",
        "        #    a. Compute gradient\n",
        "        #    b. Update weights\n",
        "        #    c. Record train loss\n",
        "        #    d. Record val loss if X_val provided\n",
        "        X_train_bias = self._add_bias(X_train)\n",
        "        if X_val is not None:\n",
        "            X_val_bias = self._add_bias(X_val)\n",
        "        \n",
        "        n_features = X_train_bias.shape[1]\n",
        "        self.weights = np.zeros(n_features)\n",
        "        \n",
        "        self.train_losses = []\n",
        "        self.val_losses = []\n",
        "        \n",
        "        for _ in range(self.n_iterations):\n",
        "            gradient = self._compute_gradient(X_train_bias, y_train)\n",
        "            self.weights -= self.learning_rate * gradient\n",
        "            \n",
        "            train_loss = self._compute_loss(X_train_bias, y_train)\n",
        "            self.train_losses.append(train_loss)\n",
        "            \n",
        "            if X_val is not None:\n",
        "                val_loss = self._compute_loss(X_val_bias, y_val)\n",
        "                self.val_losses.append(val_loss)\n",
        "        \n",
        "        return self\n",
        "    \n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Make predictions.\n",
        "        \n",
        "        Args:\n",
        "            X: Features\n",
        "        \n",
        "        Returns:\n",
        "            Predictions\n",
        "        \"\"\"\n",
        "        # TODO: Add bias and compute predictions (3 points)\n",
        "        X_bias = self._add_bias(X)\n",
        "        return X_bias @ self.weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "linreg_cv_header"
      },
      "source": [
        "## 1.2: 5-Fold Cross-Validation for Linear Regression (10 points)\n",
        "\n",
        "**Steps:**\n",
        "1. Prepare data: Use features [0, 1, 2] to predict feature [3] (petal_width)\n",
        "2. For each fold:\n",
        "   - Split train into train_inner and validation (80%-20%)\n",
        "   - Grid search over hyperparameters using validation set\n",
        "   - Train final model with best hyperparameters on full train set\n",
        "   - Evaluate on test set\n",
        "3. Report results with mean, std, and 95% CI\n",
        "4. Plot train/val loss curves for one fold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "linreg_cv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Linear Regression Data:\n",
            "  X shape: (150, 3)\n",
            "  y shape: (150,)\n",
            "\n",
            "============================================================\n",
            "LINEAR REGRESSION: 5-FOLD CROSS-VALIDATION\n",
            "============================================================\n",
            "\n",
            "--- Fold 1 ---\n",
            "Best params: lr=0.1, reg=0.01\n",
            "Test MSE: 0.0449, R²: 0.9293\n",
            "\n",
            "--- Fold 2 ---\n",
            "Best params: lr=0.1, reg=0.01\n",
            "Test MSE: 0.0248, R²: 0.9535\n",
            "\n",
            "--- Fold 3 ---\n",
            "Best params: lr=0.1, reg=0.01\n",
            "Test MSE: 0.0357, R²: 0.9368\n",
            "\n",
            "--- Fold 4 ---\n",
            "Best params: lr=0.1, reg=0.01\n",
            "Test MSE: 0.0458, R²: 0.9228\n",
            "\n",
            "--- Fold 5 ---\n",
            "Best params: lr=0.1, reg=0.01\n",
            "Test MSE: 0.0418, R²: 0.9121\n",
            "\n",
            "============================================================\n",
            "LINEAR REGRESSION: FINAL RESULTS\n",
            "============================================================\n",
            "MSE: 0.0386 ± 0.0087 (95% CI: [0.0279, 0.0494])\n",
            "R² Score: 0.9309 ± 0.0155 (95% CI: [0.9116, 0.9502])\n"
          ]
        }
      ],
      "source": [
        "# Prepare data for Linear Regression\n",
        "# Features: sepal_length, sepal_width, petal_length\n",
        "# Target: petal_width\n",
        "X_linreg = X_full[:, :3]  # First 3 features\n",
        "y_linreg = X_full[:, 3]   # 4th feature as target\n",
        "\n",
        "# Standardise features\n",
        "scaler = StandardScaler()\n",
        "X_linreg = scaler.fit_transform(X_linreg)\n",
        "\n",
        "print(f\"Linear Regression Data:\")\n",
        "print(f\"  X shape: {X_linreg.shape}\")\n",
        "print(f\"  y shape: {y_linreg.shape}\")\n",
        "\n",
        "# Hyperparameter grid\n",
        "learning_rates = [0.001, 0.01, 0.1]\n",
        "reg_strengths = [0.01, 0.1]\n",
        "\n",
        "# Storage for results\n",
        "mse_scores = []\n",
        "r2_scores = []\n",
        "best_params_per_fold = []\n",
        "\n",
        "# For plotting (save from one fold)\n",
        "plot_train_losses = None\n",
        "plot_val_losses = None\n",
        "plot_fold = 1\n",
        "\n",
        "# 5-Fold Cross-Validation\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"LINEAR REGRESSION: 5-FOLD CROSS-VALIDATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for fold_num, (train_idx, test_idx) in enumerate(kf.split(X_linreg), 1):\n",
        "    print(f\"\\n--- Fold {fold_num} ---\")\n",
        "    \n",
        "    # Split data\n",
        "    X_train_full, X_test = X_linreg[train_idx], X_linreg[test_idx]\n",
        "    y_train_full, y_test = y_linreg[train_idx], y_linreg[test_idx]\n",
        "    \n",
        "    # TODO: Further split train into train_inner and validation (80%-20%) (1 points)\n",
        "    # Hint: Use simple index slicing or another KFold split\n",
        "    split_idx = max(1, int(0.8 * len(X_train_full)))  # Ensure at least 1 sample for training\n",
        "    X_train_inner, X_val = X_train_full[:split_idx], X_train_full[split_idx:]\n",
        "    y_train_inner, y_val = y_train_full[:split_idx], y_train_full[split_idx:]\n",
        "    \n",
        "    # Ensure validation set is not empty\n",
        "    if len(X_val) == 0:\n",
        "        # If validation set is empty, use last sample from training as validation\n",
        "        split_idx = len(X_train_full) - 1\n",
        "        X_train_inner, X_val = X_train_full[:split_idx], X_train_full[split_idx:]\n",
        "        y_train_inner, y_val = y_train_full[:split_idx], y_train_full[split_idx:]\n",
        "    \n",
        "    # TODO: Grid search over hyperparameters (3 points)\n",
        "    # For each combination of (learning_rate, reg_strength):\n",
        "    #   1. Train model on train_inner\n",
        "    #   2. Evaluate on validation set\n",
        "    #   3. Track best parameters based on validation MSE\n",
        "    best_val_mse = float('inf')\n",
        "    best_params = None\n",
        "    \n",
        "    for lr in learning_rates:\n",
        "        for reg in reg_strengths:\n",
        "            model = LinearRegression(learning_rate=lr, reg_strength=reg)\n",
        "            model.fit(X_train_inner, y_train_inner, X_val, y_val)\n",
        "            val_mse = mean_squared_error(y_val, model.predict(X_val))\n",
        "            if val_mse < best_val_mse:\n",
        "                best_val_mse = val_mse\n",
        "                best_params = (lr, reg)\n",
        "                if fold_num == plot_fold and lr == 0.01 and reg == 0.1:  # Middle values\n",
        "                    plot_train_losses = model.train_losses\n",
        "                    plot_val_losses = model.val_losses\n",
        "    \n",
        "    best_params_per_fold.append(best_params)\n",
        "    # TODO: Train final model with best parameters on full training set (1 points)\n",
        "    final_model = LinearRegression(learning_rate=best_params[0], reg_strength=best_params[1])\n",
        "    final_model.fit(X_train_full, y_train_full)\n",
        "    \n",
        "    # TODO: Evaluate on test set and store results (1 points)\n",
        "    y_pred = final_model.predict(X_test)\n",
        "    mse_scores.append(mean_squared_error(y_test, y_pred))\n",
        "    r2_scores.append(r2_score(y_test, y_pred))\n",
        "    \n",
        "    print(f\"Best params: lr={best_params[0]}, reg={best_params[1]}\")\n",
        "    print(f\"Test MSE: {mse_scores[-1]:.4f}, R²: {r2_scores[-1]:.4f}\")\n",
        "\n",
        "# TODO: Print final results with mean, std, and 95% CI (1 points)\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"LINEAR REGRESSION: FINAL RESULTS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print_results(\"MSE\", mse_scores)\n",
        "print_results(\"R² Score\", r2_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "linreg_plot"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\viper23\\AppData\\Local\\Temp\\ipykernel_14212\\813153716.py:12: UserWarning: No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
            "  plt.legend()\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# TODO: Plot training and validation loss curves (1 points)\n",
        "# Use plot_train_losses and plot_val_losses saved from fold 1\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "# TODO: Create the plot (1 points)\n",
        "if plot_train_losses is not None and plot_val_losses is not None:\n",
        "    plt.plot(plot_train_losses, label='Training Loss')\n",
        "    plt.plot(plot_val_losses, label='Validation Loss')\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('MSE Loss')\n",
        "plt.title('Linear Regression: Training vs Validation Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "question1"
      },
      "source": [
        "**Question 1:** Explain how L2 regularisation affects the weights during training. (4 points, 4-5 sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "answer1"
      },
      "source": [
        "**L2 regularization adds a penalty term to the loss function that is proportional to the square of the magnitude of the weights. This penalty encourages the model to use smaller weights, preventing overfitting by discouraging complex models. During training, the regularization term affects the gradient computation by adding a component that pushes weights toward zero, with larger weights being penalized more heavily. This leads to smoother weight updates and helps prevent the model from fitting noise in the training data. Additionally, L2 regularization improves the model's generalization performance by reducing variance while maintaining reasonable bias.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "task2_header"
      },
      "source": [
        "---\n",
        "\n",
        "# Task 2: Logistic Regression (30 points)\n",
        "\n",
        "Implement Logistic Regression using gradient descent for binary classification. You will classify `setosa` (class 0) vs `non-setosa` (classes 1 and 2).\n",
        "\n",
        "**Hyperparameters to tune:**\n",
        "- Learning rate: `[0.001, 0.01, 0.1]`\n",
        "- Regularisation strength (L2): `[0.01, 0.1]`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "logreg_class_header"
      },
      "source": [
        "## 2.1: Implement LogisticRegression Class (20 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "logreg_class"
      },
      "outputs": [],
      "source": [
        "class LogisticRegression:\n",
        "    \"\"\"\n",
        "    Logistic Regression using gradient descent for binary classification.\n",
        "    \n",
        "    Attributes:\n",
        "        learning_rate: Step size for gradient descent\n",
        "        n_iterations: Number of training iterations (fixed 500)\n",
        "        reg_strength: L2 regularisation strength\n",
        "        weights: Model weights (including bias)\n",
        "        train_losses: List of training losses per iteration\n",
        "        val_losses: List of validation losses per iteration\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, learning_rate=0.01, n_iterations=500, reg_strength=0.0):\n",
        "        \"\"\"\n",
        "        Initialise the Logistic Regression model.\n",
        "        \n",
        "        Args:\n",
        "            learning_rate: Step size for gradient descent\n",
        "            n_iterations: Number of training iterations\n",
        "            reg_strength: L2 regularisation strength\n",
        "        \"\"\"\n",
        "        # TODO: Store hyperparameters and initialise attributes\n",
        "        self.learning_rate = learning_rate\n",
        "        self.n_iterations = n_iterations\n",
        "        self.reg_strength = reg_strength\n",
        "        self.weights = None\n",
        "        self.train_losses = []\n",
        "        self.val_losses = []\n",
        "    \n",
        "    def _add_bias(self, X):\n",
        "        \"\"\"Add a column of ones for the bias term.\"\"\"\n",
        "        # TODO: Add bias column to X (2 points)\n",
        "        return np.c_[np.ones(X.shape[0]), X]\n",
        "    \n",
        "    def _sigmoid(self, z):\n",
        "        \"\"\"\n",
        "        Compute sigmoid function.\n",
        "        \n",
        "        sigmoid(z) = 1 / (1 + exp(-z))\n",
        "        \n",
        "        Hint: Clip z to avoid overflow (e.g., np.clip(z, -500, 500))\n",
        "        \"\"\"\n",
        "        # TODO: Implement sigmoid (2 points)\n",
        "        z = np.clip(z, -500, 500)\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "    \n",
        "    def _compute_loss(self, X, y):\n",
        "        \"\"\"\n",
        "        Compute binary cross-entropy loss with optional L2 regularisation.\n",
        "        \n",
        "        Loss = -(1/n) * sum(y*log(p) + (1-y)*log(1-p)) + (reg_strength/2) * sum(weights^2)\n",
        "        \n",
        "        Hint: Use np.clip on probabilities to avoid log(0)\n",
        "        Note: Do not regularise the bias term.\n",
        "        \"\"\"\n",
        "        # TODO: Implement cross-entropy loss with L2 regularisation (2 points)\n",
        "        probabilities = self._sigmoid(X @ self.weights)\n",
        "        probabilities = np.clip(probabilities, 1e-15, 1 - 1e-15)  # Avoid log(0)\n",
        "        cross_entropy = -np.mean(y * np.log(probabilities) + (1 - y) * np.log(1 - probabilities))\n",
        "        reg_loss = (self.reg_strength / 2) * np.sum(self.weights[1:] ** 2)  # Exclude bias\n",
        "        return cross_entropy + reg_loss\n",
        "    \n",
        "    def _compute_gradient(self, X, y):\n",
        "        \"\"\"\n",
        "        Compute gradient of the loss function.\n",
        "        \n",
        "        Gradient = (1/n) * X.T @ (sigmoid(X @ weights) - y) + reg_strength * weights\n",
        "        \n",
        "        Note: Do not regularise the bias term.\n",
        "        \"\"\"\n",
        "        # TODO: Implement gradient computation (2 points)\n",
        "        probabilities = self._sigmoid(X @ self.weights)\n",
        "        gradient = (1 / len(y)) * X.T @ (probabilities - y)\n",
        "        gradient[1:] += self.reg_strength * self.weights[1:]  # Don't regularize bias\n",
        "        return gradient\n",
        "    \n",
        "    def fit(self, X_train, y_train, X_val=None, y_val=None):\n",
        "        \"\"\"\n",
        "        Train the model using gradient descent.\n",
        "        \n",
        "        Args:\n",
        "            X_train: Training features\n",
        "            y_train: Training labels (0 or 1)\n",
        "            X_val: Validation features (optional)\n",
        "            y_val: Validation labels (optional)\n",
        "        \n",
        "        Returns:\n",
        "            self\n",
        "        \"\"\"\n",
        "        # TODO: Implement training loop (8 points)\n",
        "        # Steps:\n",
        "        # 1. Add bias to X_train (and X_val if provided)\n",
        "        # 2. Initialise weights to zeros\n",
        "        # 3. For each iteration:\n",
        "        #    a. Compute gradient\n",
        "        #    b. Update weights\n",
        "        #    c. Record train loss\n",
        "        #    d. Record val loss if X_val provided\n",
        "        X_train_bias = self._add_bias(X_train)\n",
        "        if X_val is not None:\n",
        "            X_val_bias = self._add_bias(X_val)\n",
        "        \n",
        "        n_features = X_train_bias.shape[1]\n",
        "        self.weights = np.zeros(n_features)\n",
        "        \n",
        "        self.train_losses = []\n",
        "        self.val_losses = []\n",
        "        \n",
        "        for _ in range(self.n_iterations):\n",
        "            gradient = self._compute_gradient(X_train_bias, y_train)\n",
        "            self.weights -= self.learning_rate * gradient\n",
        "            \n",
        "            train_loss = self._compute_loss(X_train_bias, y_train)\n",
        "            self.train_losses.append(train_loss)\n",
        "            \n",
        "            if X_val is not None:\n",
        "                val_loss = self._compute_loss(X_val_bias, y_val)\n",
        "                self.val_losses.append(val_loss)\n",
        "        \n",
        "        return self\n",
        "    \n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"\n",
        "        Predict probabilities.\n",
        "        \n",
        "        Args:\n",
        "            X: Features\n",
        "        \n",
        "        Returns:\n",
        "            Probabilities of class 1\n",
        "        \"\"\"\n",
        "        # TODO: Add bias and compute probabilities using sigmoid (2 points)\n",
        "        X_bias = self._add_bias(X)\n",
        "        return self._sigmoid(X_bias @ self.weights)\n",
        "    \n",
        "    def predict(self, X, threshold=0.5):\n",
        "        \"\"\"\n",
        "        Make class predictions.\n",
        "        \n",
        "        Args:\n",
        "            X: Features\n",
        "            threshold: Classification threshold (default 0.5)\n",
        "        \n",
        "        Returns:\n",
        "            Class predictions (0 or 1)\n",
        "        \"\"\"\n",
        "        # TODO: Get probabilities and apply threshold (2 points)\n",
        "        probabilities = self.predict_proba(X)\n",
        "        return (probabilities >= threshold).astype(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "logreg_cv_header"
      },
      "source": [
        "## 2.2: 5-Fold Cross-Validation for Logistic Regression (10 points)\n",
        "\n",
        "**Steps:**\n",
        "1. Prepare data: Binary labels (setosa=1, non-setosa=0)\n",
        "2. For each fold:\n",
        "   - Split train into train_inner and validation (80%-20%)\n",
        "   - Grid search over hyperparameters using validation set\n",
        "   - Train final model with best hyperparameters on full train set\n",
        "   - Evaluate on test set\n",
        "3. Report results with mean, std, and 95% CI\n",
        "4. Plot train/val loss curves for one fold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "logreg_cv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logistic Regression Data:\n",
            "  X shape: (150, 4)\n",
            "  y shape: (150,)\n",
            "  Class distribution: {np.int64(1): 50, np.int64(0): 100}\n",
            "\n",
            "============================================================\n",
            "LOGISTIC REGRESSION: 5-FOLD CROSS-VALIDATION\n",
            "============================================================\n",
            "\n",
            "--- Fold 1 ---\n",
            "  Train inner: 96 samples, Validation: 24 samples\n",
            "Best params: lr=0.001, reg=0.01\n",
            "Test Acc: 1.0000, Prec: 1.0000, Rec: 1.0000, F1: 1.0000\n",
            "\n",
            "--- Fold 2 ---\n",
            "  Train inner: 96 samples, Validation: 24 samples\n",
            "Best params: lr=0.001, reg=0.01\n",
            "Test Acc: 1.0000, Prec: 1.0000, Rec: 1.0000, F1: 1.0000\n",
            "\n",
            "--- Fold 3 ---\n",
            "  Train inner: 96 samples, Validation: 24 samples\n",
            "Best params: lr=0.001, reg=0.01\n",
            "Test Acc: 1.0000, Prec: 1.0000, Rec: 1.0000, F1: 1.0000\n",
            "\n",
            "--- Fold 4 ---\n",
            "  Train inner: 96 samples, Validation: 24 samples\n",
            "Best params: lr=0.001, reg=0.01\n",
            "Test Acc: 1.0000, Prec: 1.0000, Rec: 1.0000, F1: 1.0000\n",
            "\n",
            "--- Fold 5 ---\n",
            "  Train inner: 96 samples, Validation: 24 samples\n",
            "Best params: lr=0.001, reg=0.01\n",
            "Test Acc: 1.0000, Prec: 1.0000, Rec: 1.0000, F1: 1.0000\n",
            "\n",
            "============================================================\n",
            "LOGISTIC REGRESSION: FINAL RESULTS\n",
            "============================================================\n",
            "Accuracy: 1.0000 ± 0.0000 (95% CI: [1.0000, 1.0000])\n",
            "Precision: 1.0000 ± 0.0000 (95% CI: [1.0000, 1.0000])\n",
            "Recall: 1.0000 ± 0.0000 (95% CI: [1.0000, 1.0000])\n",
            "F1 Score: 1.0000 ± 0.0000 (95% CI: [1.0000, 1.0000])\n"
          ]
        }
      ],
      "source": [
        "# Prepare data for Logistic Regression\n",
        "# Binary classification: setosa (0) vs non-setosa (1, 2)\n",
        "X_logreg = X_full.copy()\n",
        "y_logreg = (y_full == 0).astype(int)  # 1 for setosa, 0 for others\n",
        "\n",
        "# Standardise features\n",
        "scaler = StandardScaler()\n",
        "X_logreg = scaler.fit_transform(X_logreg)\n",
        "\n",
        "print(f\"Logistic Regression Data:\")\n",
        "print(f\"  X shape: {X_logreg.shape}\")\n",
        "print(f\"  y shape: {y_logreg.shape}\")\n",
        "print(f\"  Class distribution: {dict(Counter(y_logreg))}\")\n",
        "\n",
        "# Hyperparameter grid\n",
        "learning_rates = [0.001, 0.01, 0.1]\n",
        "reg_strengths = [0.01, 0.1]\n",
        "\n",
        "# Storage for results\n",
        "accuracy_scores = []\n",
        "precision_scores_list = []\n",
        "recall_scores_list = []\n",
        "f1_scores_list = []\n",
        "best_params_per_fold = []\n",
        "\n",
        "# For plotting (save from one fold)\n",
        "plot_train_losses = None\n",
        "plot_val_losses = None\n",
        "\n",
        "# 5-Fold Cross-Validation\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"LOGISTIC REGRESSION: 5-FOLD CROSS-VALIDATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for fold_num, (train_idx, test_idx) in enumerate(kf.split(X_logreg), 1):\n",
        "    print(f\"\\n--- Fold {fold_num} ---\")\n",
        "    \n",
        "    # Split data\n",
        "    X_train_full, X_test = X_logreg[train_idx], X_logreg[test_idx]\n",
        "    y_train_full, y_test = y_logreg[train_idx], y_logreg[test_idx]\n",
        "    \n",
        "    # TODO: Further split train into train_inner and validation (80%-20%)\n",
        "    split_idx = max(1, int(0.8 * len(X_train_full)))  # Ensure at least 1 sample for training\n",
        "    X_train_inner, X_val = X_train_full[:split_idx], X_train_full[split_idx:]\n",
        "    y_train_inner, y_val = y_train_full[:split_idx], y_train_full[split_idx:]\n",
        "    \n",
        "    # Ensure validation set is not empty\n",
        "    if len(X_val) == 0:\n",
        "        # If validation set is empty, use last sample from training as validation\n",
        "        split_idx = len(X_train_full) - 1\n",
        "        X_train_inner, X_val = X_train_full[:split_idx], X_train_full[split_idx:]\n",
        "        y_train_inner, y_val = y_train_full[:split_idx], y_train_full[split_idx:]\n",
        "    \n",
        "    print(f\"  Train inner: {X_train_inner.shape[0]} samples, Validation: {X_val.shape[0]} samples\")\n",
        "    \n",
        "    # TODO: Grid search over hyperparameters (3 points)\n",
        "    # For each combination of (learning_rate, reg_strength):\n",
        "    #   1. Train model on train_inner\n",
        "    #   2. Evaluate on validation set (use accuracy for selection)\n",
        "   #   3. Track best parameters\n",
        "    best_val_acc = 0\n",
        "    best_params = (0.01, 0.01)  # Default fallback\n",
        "    \n",
        "    try:\n",
        "        for lr in learning_rates:\n",
        "            for reg in reg_strengths:\n",
        "                model = LogisticRegression(learning_rate=lr, reg_strength=reg)\n",
        "                model.fit(X_train_inner, y_train_inner, X_val, y_val)\n",
        "                val_pred = model.predict(X_val)\n",
        "                val_acc = accuracy_score(y_val, val_pred)\n",
        "                if val_acc > best_val_acc:\n",
        "                    best_val_acc = val_acc\n",
        "                    best_params = (lr, reg)\n",
        "                    if fold_num == 1 and lr == 0.01 and reg == 0.1:  # Middle values\n",
        "                        plot_train_losses = model.train_losses.copy()\n",
        "                        plot_val_losses = model.val_losses.copy()\n",
        "    except Exception as e:\n",
        "        print(f\"  Warning: Grid search failed for fold {fold_num}: {e}\")\n",
        "        print(f\"  Using default parameters: lr={best_params[0]}, reg={best_params[1]}\")\n",
        "    \n",
        "    best_params_per_fold.append(best_params)\n",
        "    # TODO: Save train/val losses from one hyperparameter setting for plotting (1 points)\n",
        "    \n",
        "    # TODO: Train final model with best parameters on full training set (1 points)\n",
        "    final_model = LogisticRegression(learning_rate=best_params[0], reg_strength=best_params[1])\n",
        "    final_model.fit(X_train_full, y_train_full)\n",
        "    \n",
        "    # TODO: Evaluate on test set and store results (2 points)\n",
        "    # Store: accuracy, precision, recall, f1\n",
        "    y_pred = final_model.predict(X_test)\n",
        "    accuracy_scores.append(accuracy_score(y_test, y_pred))\n",
        "    precision_scores_list.append(precision_score(y_test, y_pred))\n",
        "    recall_scores_list.append(recall_score(y_test, y_pred))\n",
        "    f1_scores_list.append(f1_score(y_test, y_pred))\n",
        "    \n",
        "    print(f\"Best params: lr={best_params[0]}, reg={best_params[1]}\")\n",
        "    print(f\"Test Acc: {accuracy_scores[-1]:.4f}, Prec: {precision_scores_list[-1]:.4f}, Rec: {recall_scores_list[-1]:.4f}, F1: {f1_scores_list[-1]:.4f}\")\n",
        "\n",
        "# TODO: Print final results with mean, std, and 95% CI for all metrics (1 points)\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"LOGISTIC REGRESSION: FINAL RESULTS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print_results(\"Accuracy\", accuracy_scores)\n",
        "print_results(\"Precision\", precision_scores_list)\n",
        "print_results(\"Recall\", recall_scores_list)\n",
        "print_results(\"F1 Score\", f1_scores_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "logreg_plot"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\viper23\\AppData\\Local\\Temp\\ipykernel_14212\\3966894041.py:11: UserWarning: No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
            "  plt.legend()\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# TODO: Plot training and validation loss curves (1 points)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "# TODO: Create the plot  (1 points)\n",
        "if plot_train_losses is not None and plot_val_losses is not None:\n",
        "    plt.plot(plot_train_losses, label='Training Loss')\n",
        "    plt.plot(plot_val_losses, label='Validation Loss')\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Cross-Entropy Loss')\n",
        "plt.title('Logistic Regression: Training vs Validation Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "question2"
      },
      "source": [
        "**Question 2:** Why do we use the sigmoid function in Logistic Regression? (4 points, 4-5 sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "answer2"
      },
      "source": [
        "**The sigmoid function is used in Logistic Regression to transform the linear combination of features into a probability value between 0 and 1. This transformation allows the model to output probabilities that can be interpreted as the likelihood of belonging to the positive class. The sigmoid function's S-shaped curve ensures that small changes in the input near the decision boundary result in significant changes in probability, while large positive or negative inputs are mapped to probabilities close to 1 or 0 respectively. This probabilistic output enables threshold-based classification and provides meaningful confidence scores for predictions.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "task3_header"
      },
      "source": [
        "---\n",
        "\n",
        "# Task 3: Decision Tree (30 points)\n",
        "\n",
        "Implement a Decision Tree classifier using information gain (entropy) for multi-class classification of all three Iris species.\n",
        "\n",
        "**Hyperparameters to tune:**\n",
        "- Maximum depth: `[3, 5, 10]`\n",
        "- Minimum samples to split: `[2, 5, 10]`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tree_class_header"
      },
      "source": [
        "## 3.1: Implement DecisionTree Class (20 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "tree_class"
      },
      "outputs": [],
      "source": [
        "class DecisionTreeNode:\n",
        "    \"\"\"A node in the decision tree.\"\"\"\n",
        "    \n",
        "    def __init__(self, feature_idx=None, threshold=None, left=None, right=None, value=None):\n",
        "        \"\"\"\n",
        "        Initialise a tree node.\n",
        "        \n",
        "        For internal nodes:\n",
        "            feature_idx: Index of feature to split on\n",
        "            threshold: Threshold value for the split\n",
        "            left: Left child node (feature <= threshold)\n",
        "            right: Right child node (feature > threshold)\n",
        "        \n",
        "        For leaf nodes:\n",
        "            value: Predicted class label\n",
        "        \"\"\"\n",
        "        self.feature_idx = feature_idx\n",
        "        self.threshold = threshold\n",
        "        self.left = left\n",
        "        self.right = right\n",
        "        self.value = value\n",
        "    \n",
        "    def is_leaf(self):\n",
        "        \"\"\"Check if this node is a leaf.\"\"\"\n",
        "        return self.value is not None\n",
        "\n",
        "\n",
        "class DecisionTree:\n",
        "    \"\"\"\n",
        "    Decision Tree classifier using information gain (entropy).\n",
        "    \n",
        "    Attributes:\n",
        "        max_depth: Maximum depth of the tree\n",
        "        min_samples_split: Minimum samples required to split a node\n",
        "        min_samples_leaf: Minimum samples required in a leaf node\n",
        "        root: Root node of the tree\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, max_depth=10, min_samples_split=2, min_samples_leaf=1):\n",
        "        \"\"\"\n",
        "        Initialise the Decision Tree.\n",
        "        \n",
        "        Args:\n",
        "            max_depth: Maximum depth of the tree\n",
        "            min_samples_split: Minimum samples required to split\n",
        "            min_samples_leaf: Minimum samples required in a leaf (fixed 1)\n",
        "        \"\"\"\n",
        "        # TODO: Store hyperparameters (1 points)\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.min_samples_leaf = min_samples_leaf\n",
        "        self.root = None\n",
        "    \n",
        "    def _entropy(self, y):\n",
        "        \"\"\"\n",
        "        Compute entropy of a label array.\n",
        "        \n",
        "        Entropy = -sum(p * log2(p)) for each class\n",
        "        \n",
        "        Hint: Handle the case where p=0 (0 * log(0) = 0)\n",
        "        \"\"\"\n",
        "        # TODO: Implement entropy calculation (2 points)\n",
        "        if len(y) == 0:\n",
        "            return 0\n",
        "        \n",
        "        _, counts = np.unique(y, return_counts=True)\n",
        "        probabilities = counts / len(y)\n",
        "        entropy = 0\n",
        "        for p in probabilities:\n",
        "            if p > 0:\n",
        "                entropy -= p * np.log2(p)\n",
        "        return entropy\n",
        "    \n",
        "    def _information_gain(self, y, y_left, y_right):\n",
        "        \"\"\"\n",
        "        Compute information gain from a split.\n",
        "        \n",
        "        IG = entropy(parent) - weighted_avg(entropy(children))\n",
        "        \"\"\"\n",
        "        # TODO: Implement information gain (2 points)\n",
        "        parent_entropy = self._entropy(y)\n",
        "        n = len(y)\n",
        "        n_left, n_right = len(y_left), len(y_right)\n",
        "        \n",
        "        if n_left == 0 or n_right == 0:\n",
        "            return 0\n",
        "        \n",
        "        weighted_entropy = (n_left / n) * self._entropy(y_left) + (n_right / n) * self._entropy(y_right)\n",
        "        return parent_entropy - weighted_entropy\n",
        "    \n",
        "    def _best_split(self, X, y):\n",
        "        \"\"\"\n",
        "        Find the best feature and threshold to split on.\n",
        "        \n",
        "        Returns:\n",
        "            best_feature_idx, best_threshold, best_gain\n",
        "            Returns (None, None, 0) if no valid split found\n",
        "        \"\"\"\n",
        "        # TODO: Implement best split search (5 points)\n",
        "        # Steps:\n",
        "        # 1. For each feature:\n",
        "        #    a. Get unique values as potential thresholds\n",
        "        #    b. For each threshold:\n",
        "        #       - Split data into left (<=) and right (>)\n",
        "        #       - Check min_samples_leaf constraint\n",
        "        #       - Compute information gain\n",
        "        #       - Track best split\n",
        "        # 2. Return best feature, threshold, and gain\n",
        "        best_gain = 0\n",
        "        best_feature_idx = None\n",
        "        best_threshold = None\n",
        "        \n",
        "        n_samples, n_features = X.shape\n",
        "        \n",
        "        for feature_idx in range(n_features):\n",
        "            feature_values = X[:, feature_idx]\n",
        "            unique_values = np.unique(feature_values)\n",
        "            \n",
        "            for threshold in unique_values:\n",
        "                left_mask = feature_values <= threshold\n",
        "                right_mask = feature_values > threshold\n",
        "                \n",
        "                y_left, y_right = y[left_mask], y[right_mask]\n",
        "                \n",
        "                if len(y_left) < self.min_samples_leaf or len(y_right) < self.min_samples_leaf:\n",
        "                    continue\n",
        "                \n",
        "                gain = self._information_gain(y, y_left, y_right)\n",
        "                \n",
        "                if gain > best_gain:\n",
        "                    best_gain = gain\n",
        "                    best_feature_idx = feature_idx\n",
        "                    best_threshold = threshold\n",
        "        \n",
        "        return best_feature_idx, best_threshold, best_gain\n",
        "    \n",
        "    def _build_tree(self, X, y, depth=0):\n",
        "        \"\"\"\n",
        "        Recursively build the decision tree.\n",
        "        \n",
        "        Args:\n",
        "            X: Features\n",
        "            y: Labels\n",
        "            depth: Current depth\n",
        "        \n",
        "        Returns:\n",
        "            DecisionTreeNode\n",
        "        \"\"\"\n",
        "        # TODO: Implement recursive tree building (6 points)\n",
        "        # Steps:\n",
        "        # 1. Check stopping conditions:\n",
        "        #    - max_depth reached\n",
        "        #    - all samples same class\n",
        "        #    - n_samples < min_samples_split\n",
        "        #    If stopping, return leaf with majority class\n",
        "        #\n",
        "        # 2. Find best split\n",
        "        #    If no valid split, return leaf\n",
        "        #\n",
        "        # 3. Split data and recursively build children\n",
        "        #\n",
        "        # 4. Return internal node with split info\n",
        "        n_samples = len(y)\n",
        "        \n",
        "        # Check stopping conditions\n",
        "        if (depth >= self.max_depth or \n",
        "            len(np.unique(y)) == 1 or \n",
        "            n_samples < self.min_samples_split):\n",
        "            # Return leaf with majority class\n",
        "            majority_class = np.bincount(y).argmax()\n",
        "            return DecisionTreeNode(value=majority_class)\n",
        "        \n",
        "        # Find best split\n",
        "        best_feature_idx, best_threshold, best_gain = self._best_split(X, y)\n",
        "        \n",
        "        if best_gain == 0:\n",
        "            # No valid split found\n",
        "            majority_class = np.bincount(y).argmax()\n",
        "            return DecisionTreeNode(value=majority_class)\n",
        "        \n",
        "        # Split data\n",
        "        left_mask = X[:, best_feature_idx] <= best_threshold\n",
        "        right_mask = X[:, best_feature_idx] > best_threshold\n",
        "        \n",
        "        X_left, y_left = X[left_mask], y[left_mask]\n",
        "        X_right, y_right = X[right_mask], y[right_mask]\n",
        "        \n",
        "        # Recursively build children\n",
        "        left_child = self._build_tree(X_left, y_left, depth + 1)\n",
        "        right_child = self._build_tree(X_right, y_right, depth + 1)\n",
        "        \n",
        "        # Return internal node\n",
        "        return DecisionTreeNode(\n",
        "            feature_idx=best_feature_idx,\n",
        "            threshold=best_threshold,\n",
        "            left=left_child,\n",
        "            right=right_child\n",
        "        )\n",
        "    \n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Train the decision tree.\n",
        "        \n",
        "        Args:\n",
        "            X: Training features\n",
        "            y: Training labels\n",
        "        \n",
        "        Returns:\n",
        "            self\n",
        "        \"\"\"\n",
        "        # TODO: Build the tree (2 points)\n",
        "        self.root = self._build_tree(X, y)\n",
        "        return self\n",
        "    \n",
        "    def _predict_sample(self, x, node):\n",
        "        \"\"\"\n",
        "        Predict class for a single sample by traversing the tree.\n",
        "        \"\"\"\n",
        "        # TODO: Traverse tree to make prediction (1 points)\n",
        "        if node.is_leaf():\n",
        "            return node.value\n",
        "        \n",
        "        if x[node.feature_idx] <= node.threshold:\n",
        "            return self._predict_sample(x, node.left)\n",
        "        else:\n",
        "            return self._predict_sample(x, node.right)\n",
        "    \n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Make predictions for all samples.\n",
        "        \n",
        "        Args:\n",
        "            X: Features\n",
        "        \n",
        "        Returns:\n",
        "            Array of predictions\n",
        "        \"\"\"\n",
        "        # TODO: Predict for each sample (1 points)\n",
        "        return np.array([self._predict_sample(x, self.root) for x in X])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tree_cv_header"
      },
      "source": [
        "## 3.2: 5-Fold Cross-Validation for Decision Tree (10 points)\n",
        "\n",
        "**Steps:**\n",
        "1. Prepare data: Multi-class classification (all 3 classes)\n",
        "2. For each fold:\n",
        "   - Split train into train_inner and validation (80%-20%)\n",
        "   - Grid search over hyperparameters using validation set\n",
        "   - Train final model with best hyperparameters on full train set\n",
        "   - Evaluate on test set\n",
        "3. Report results with mean, std, and 95% CI\n",
        "4. Plot validation accuracy for different max_depth values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "tree_cv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Decision Tree Data:\n",
            "  X shape: (150, 4)\n",
            "  y shape: (150,)\n",
            "  Classes: [np.str_('setosa'), np.str_('versicolor'), np.str_('virginica')]\n",
            "\n",
            "============================================================\n",
            "DECISION TREE: 5-FOLD CROSS-VALIDATION\n",
            "============================================================\n",
            "\n",
            "--- Fold 1 ---\n",
            "Best params: max_depth=5, min_split=2, min_leaf=1\n",
            "Test Acc: 1.0000, Prec: 1.0000, Rec: 1.0000, F1: 1.0000\n",
            "\n",
            "--- Fold 2 ---\n",
            "Best params: max_depth=3, min_split=2, min_leaf=4\n",
            "Test Acc: 1.0000, Prec: 1.0000, Rec: 1.0000, F1: 1.0000\n",
            "\n",
            "--- Fold 3 ---\n",
            "Best params: max_depth=3, min_split=2, min_leaf=1\n",
            "Test Acc: 0.9333, Prec: 0.9333, Rec: 0.9333, F1: 0.9259\n",
            "\n",
            "--- Fold 4 ---\n",
            "Best params: max_depth=5, min_split=2, min_leaf=1\n",
            "Test Acc: 0.9333, Prec: 0.9389, Rec: 0.9389, F1: 0.9389\n",
            "\n",
            "--- Fold 5 ---\n",
            "Best params: max_depth=3, min_split=2, min_leaf=4\n",
            "Test Acc: 0.9667, Prec: 0.9722, Rec: 0.9722, F1: 0.9710\n",
            "\n",
            "============================================================\n",
            "DECISION TREE: FINAL RESULTS\n",
            "============================================================\n",
            "Accuracy: 0.9667 ± 0.0333 (95% CI: [0.9253, 1.0080])\n",
            "Precision (Macro): 0.9689 ± 0.0321 (95% CI: [0.9291, 1.0087])\n",
            "Recall (Macro): 0.9689 ± 0.0321 (95% CI: [0.9291, 1.0087])\n",
            "F1 Score (Macro): 0.9672 ± 0.0342 (95% CI: [0.9247, 1.0096])\n"
          ]
        }
      ],
      "source": [
        "# Prepare data for Decision Tree\n",
        "# Multi-class classification: all 3 classes\n",
        "X_tree = X_full.copy()\n",
        "y_tree = y_full.copy()\n",
        "\n",
        "print(f\"Decision Tree Data:\")\n",
        "print(f\"  X shape: {X_tree.shape}\")\n",
        "print(f\"  y shape: {y_tree.shape}\")\n",
        "print(f\"  Classes: {list(iris.target_names)}\")\n",
        "\n",
        "# Hyperparameter grid\n",
        "max_depths = [3, 5, 10]\n",
        "min_samples_leafs = [1, 2, 4]\n",
        "\n",
        "# Storage for results\n",
        "accuracy_scores = []\n",
        "precision_scores_list = []\n",
        "recall_scores_list = []\n",
        "f1_scores_list = []\n",
        "best_params_per_fold = []\n",
        "\n",
        "# For plotting (track validation accuracy vs max_depth from one fold)\n",
        "depth_val_accuracies = {d: [] for d in max_depths}\n",
        "\n",
        "# 5-Fold Cross-Validation\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"DECISION TREE: 5-FOLD CROSS-VALIDATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for fold_num, (train_idx, test_idx) in enumerate(kf.split(X_tree), 1):\n",
        "    print(f\"\\n--- Fold {fold_num} ---\")\n",
        "    \n",
        "    # Split data\n",
        "    X_train_full, X_test = X_tree[train_idx], X_tree[test_idx]\n",
        "    y_train_full, y_test = y_tree[train_idx], y_tree[test_idx]\n",
        "    \n",
        "    # TODO: Further split train into train_inner and validation (80%-20%)\n",
        "    split_idx = max(1, int(0.8 * len(X_train_full)))  # Ensure at least 1 sample for training\n",
        "    X_train_inner, X_val = X_train_full[:split_idx], X_train_full[split_idx:]\n",
        "    y_train_inner, y_val = y_train_full[:split_idx], y_train_full[split_idx:]\n",
        "    \n",
        "    # Ensure validation set is not empty\n",
        "    if len(X_val) == 0:\n",
        "        # If validation set is empty, use last sample from training as validation\n",
        "        split_idx = len(X_train_full) - 1\n",
        "        X_train_inner, X_val = X_train_full[:split_idx], X_train_full[split_idx:]\n",
        "        y_train_inner, y_val = y_train_full[:split_idx], y_train_full[split_idx:]\n",
        "    \n",
        "    # TODO: Grid search over hyperparameters (5 points)\n",
        "    # For each combination of (max_depth, min_samples_split, min_samples_leaf):\n",
        "    #   1. Train model on train_inner\n",
        "    #   2. Evaluate on validation set\n",
        "    #   3. Track best parameters\n",
        "    #   4. For plotting: track average val accuracy for each max_depth\n",
        "    best_val_acc = 0\n",
        "    best_params = None\n",
        "    \n",
        "    for max_depth in max_depths:\n",
        "        fold_depth_acc = []\n",
        "        for min_samples_split in [2, 5, 10]:\n",
        "            for min_samples_leaf in min_samples_leafs:\n",
        "                model = DecisionTree(max_depth=max_depth, min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf)\n",
        "                model.fit(X_train_inner, y_train_inner)\n",
        "                val_pred = model.predict(X_val)\n",
        "                val_acc = accuracy_score(y_val, val_pred)\n",
        "                fold_depth_acc.append(val_acc)\n",
        "                \n",
        "                if val_acc > best_val_acc:\n",
        "                    best_val_acc = val_acc\n",
        "                    best_params = (max_depth, min_samples_split, min_samples_leaf)\n",
        "        \n",
        "        # Track average accuracy for this max_depth\n",
        "        depth_val_accuracies[max_depth].append(np.mean(fold_depth_acc))\n",
        "    \n",
        "    best_params_per_fold.append(best_params)\n",
        "    # TODO: Train final model with best parameters on full training set (1 points)\n",
        "    final_model = DecisionTree(max_depth=best_params[0], min_samples_split=best_params[1], min_samples_leaf=best_params[2])\n",
        "    final_model.fit(X_train_full, y_train_full)\n",
        "    \n",
        "    # TODO: Evaluate on test set and store results (1 points)\n",
        "    # Store: accuracy, precision (macro), recall (macro), f1 (macro)\n",
        "    y_pred = final_model.predict(X_test)\n",
        "    accuracy_scores.append(accuracy_score(y_test, y_pred))\n",
        "    precision_scores_list.append(precision_score(y_test, y_pred, average='macro'))\n",
        "    recall_scores_list.append(recall_score(y_test, y_pred, average='macro'))\n",
        "    f1_scores_list.append(f1_score(y_test, y_pred, average='macro'))\n",
        "    \n",
        "    print(f\"Best params: max_depth={best_params[0]}, min_split={best_params[1]}, min_leaf={best_params[2]}\")\n",
        "    print(f\"Test Acc: {accuracy_scores[-1]:.4f}, Prec: {precision_scores_list[-1]:.4f}, Rec: {recall_scores_list[-1]:.4f}, F1: {f1_scores_list[-1]:.4f}\")\n",
        "\n",
        "# TODO: Print final results with mean, std, and 95% CI for all metrics (1 points)\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"DECISION TREE: FINAL RESULTS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print_results(\"Accuracy\", accuracy_scores)\n",
        "print_results(\"Precision (Macro)\", precision_scores_list)\n",
        "print_results(\"Recall (Macro)\", recall_scores_list)\n",
        "print_results(\"F1 Score (Macro)\", f1_scores_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "tree_plot"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# TODO: Plot validation accuracy vs max_depth (1 points)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "# TODO: Create the plot (1 points)\n",
        "plt.plot(max_depths, [np.mean(depth_val_accuracies[d]) for d in max_depths], marker='o')\n",
        "plt.xlabel('Max Depth')\n",
        "plt.ylabel('Validation Accuracy')\n",
        "plt.title('Decision Tree: Validation Accuracy vs Max Depth')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "question3"
      },
      "source": [
        "**Question 3:** What are the risks of having a tree that is too shallow or too deep? (4 points, 4-5 sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "answer3"
      },
      "source": [
        "**A tree that is too shallow (low maximum depth) risks underfitting the training data, resulting in poor performance on both training and test sets. This occurs because the model cannot capture complex patterns in the data, leading to high bias. Conversely, a tree that is too deep risks overfitting, where the model memorizes noise and specific training examples rather than learning general patterns. This results in excellent performance on training data but poor generalization to unseen test data, manifesting as high variance. The optimal tree depth balances these trade-offs, achieving good generalization performance.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdf_header"
      },
      "source": [
        "---\n",
        "\n",
        "# Convert Your Colab Notebook to PDF\n",
        "\n",
        "### Step 1: Download Your Notebook\n",
        "- Go to **File → Download → Download .ipynb**\n",
        "- Save the file to your computer\n",
        "\n",
        "### Step 2: Upload to Colab\n",
        "- Click the **📁 folder icon** on the left sidebar\n",
        "- Click the **upload button**\n",
        "- Select your downloaded .ipynb file\n",
        "- Wait for the upload to complete\n",
        "\n",
        "### Step 3: Run the Code Below\n",
        "- **Uncomment the cell below** and run the cell\n",
        "- This will take about 1-2 minutes to install required packages\n",
        "\n",
        "### Step 4: Enter Notebook Name\n",
        "- When prompted, type your notebook name (e.g., `gs_000000_project.ipynb`)\n",
        "- Press Enter\n",
        "\n",
        "### The PDF will automatically download to your computer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "pdf_code"
      },
      "outputs": [],
      "source": [
        "# # Install required packages (this takes about 30 seconds)\n",
        "# print(\"Installing PDF converter... please wait...\")\n",
        "# !apt-get update -qq\n",
        "# !apt-get install -y texlive-xetex texlive-fonts-recommended texlive-plain-generic pandoc > /dev/null 2>&1\n",
        "# !pip install -q nbconvert\n",
        "\n",
        "# print(\"\\n\" + \"=\"*50)\n",
        "# print(\"COLAB NOTEBOOK TO PDF CONVERTER\")\n",
        "# print(\"=\"*50)\n",
        "# print(\"\\nSTEP 1: Download your notebook\")\n",
        "# print(\"- Go to File → Download → Download .ipynb\")\n",
        "# print(\"- Save it to your computer\")\n",
        "# print(\"\\nSTEP 2: Upload it here\")\n",
        "# print(\"- Click the folder icon on the left (📁)\")\n",
        "# print(\"- Click the upload button and select your .ipynb file\")\n",
        "# print(\"- Wait for upload to complete\")\n",
        "# print(\"\\nSTEP 3: Enter the filename below\")\n",
        "# print(\"=\"*50)\n",
        "\n",
        "# # Get notebook name from user\n",
        "# notebook_name = input(\"\\nEnter your notebook name: \")\n",
        "\n",
        "# # Add .ipynb if missing\n",
        "# if not notebook_name.endswith('.ipynb'):\n",
        "#     notebook_name += '.ipynb'\n",
        "\n",
        "# import os\n",
        "# notebook_path = f'/content/{notebook_name}'\n",
        "\n",
        "# # Check if file exists\n",
        "# if not os.path.exists(notebook_path):\n",
        "#     print(f\"\\n⚠ Error: '{notebook_name}' not found in /content/\")\n",
        "#     print(\"\\nMake sure you uploaded the file using the folder icon (📁) on the left!\")\n",
        "# else:\n",
        "#     print(f\"\\n✔ Found {notebook_name}\")\n",
        "#     print(\"Converting to PDF... this may take 1-2 minutes...\\n\")\n",
        "\n",
        "#     # Convert the notebook to PDF\n",
        "#     !jupyter nbconvert --to pdf \"{notebook_path}\"\n",
        "\n",
        "#     # Download the PDF\n",
        "#     from google.colab import files\n",
        "#     pdf_name = notebook_name.replace('.ipynb', '.pdf')\n",
        "#     pdf_path = f'/content/{pdf_name}'\n",
        "\n",
        "#     if os.path.exists(pdf_path):\n",
        "#         print(\"✔ SUCCESS! Downloading your PDF now...\")\n",
        "#         files.download(pdf_path)\n",
        "#         print(\"\\n✔ Done! Check your downloads folder.\")\n",
        "#     else:\n",
        "#         print(\"⚠ Error: Could not create PDF\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
